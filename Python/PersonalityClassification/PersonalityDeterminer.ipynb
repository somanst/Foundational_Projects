{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674158247.5824366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10224\\2443852643.py:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataArray = np.array(list(map(norm, dataArray)))\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from IPython.display import display\n",
    "import time\n",
    "start = time.time()\n",
    "print(start)\n",
    "k_fold = 5\n",
    "_k = 7\n",
    "f_norm = True\n",
    "weight = True\n",
    "def norm(num):\n",
    "    num = (num + 3)/(3 + 3)\n",
    "    return num\n",
    "                        \n",
    "df = pd.read_csv(\"16P.csv\", encoding='cp1252').drop([\"Response Id\"], axis = 1)\n",
    "df.loc[df.Personality == \"ESTJ\", \"Personality\"] = 0\n",
    "df.loc[df.Personality == \"ENTJ\", \"Personality\"] = 1\n",
    "df.loc[df.Personality == \"ESFJ\", \"Personality\"] = 2\n",
    "df.loc[df.Personality == \"ENFJ\", \"Personality\"] = 3\n",
    "df.loc[df.Personality == \"ISTJ\", \"Personality\"] = 4\n",
    "df.loc[df.Personality == \"ISFJ\", \"Personality\"] = 5\n",
    "df.loc[df.Personality == \"INTJ\", \"Personality\"] = 6\n",
    "df.loc[df.Personality == \"INFJ\", \"Personality\"] = 7\n",
    "df.loc[df.Personality == \"ESTP\", \"Personality\"] = 8\n",
    "df.loc[df.Personality == \"ESFP\", \"Personality\"] = 9\n",
    "df.loc[df.Personality == \"ENTP\", \"Personality\"] = 10\n",
    "df.loc[df.Personality == \"ENFP\", \"Personality\"] = 11\n",
    "df.loc[df.Personality == \"ISTP\", \"Personality\"] = 12\n",
    "df.loc[df.Personality == \"ISFP\", \"Personality\"] = 13\n",
    "df.loc[df.Personality == \"INTP\", \"Personality\"] = 14\n",
    "df.loc[df.Personality == \"INFP\", \"Personality\"] = 15\n",
    "dataArray = df.iloc[:,:-1].to_numpy()\n",
    "resultArray = df[[\"Personality\"]].to_numpy()\n",
    "\n",
    "dataArray = np.array_split(dataArray, k_fold)\n",
    "resultArray = np.array_split(resultArray, k_fold)\n",
    "calcs = []\n",
    "if f_norm == True:\n",
    "    dataArray = np.array(list(map(norm, dataArray)))\n",
    "for k in range(k_fold):    \n",
    "    test = dataArray[k]    \n",
    "    testResult = resultArray[k]   \n",
    "    train = np.delete(dataArray, k)\n",
    "    train = np.concatenate(train)\n",
    "    trainResult = np.delete(resultArray, k)\n",
    "    trainResult = np.concatenate(trainResult)\n",
    "    confMatrixInput = []\n",
    "    confMatrix = np.zeros((16,16))\n",
    "    confMatrix[1,0] += 1\n",
    "    tp, fp, tn, fn = 0,0,0,0\n",
    "    for i in range(len(test)):        \n",
    "        point = test[i]\n",
    "        result = int(testResult[i])\n",
    "        indivPointDistance = []\n",
    "        distance = np.sqrt(np.sum((point-train)**2,axis = 1))         \n",
    "        sortedDistances = np.argpartition(distance, _k)  \n",
    "        shortestIndex = sortedDistances[:_k]\n",
    "        if weight == True:\n",
    "            weights = np.zeros((16,))\n",
    "            for index in shortestIndex:\n",
    "                tempRes = int(trainResult[index])\n",
    "                tempDis = 1/distance[index]\n",
    "                weights[tempRes] += tempDis\n",
    "            guess = weights.argmax()\n",
    "        else:                        \n",
    "            possibilities = [trainResult[n] for n in shortestIndex]\n",
    "            personalities = np.array(possibilities, dtype = \"int64\")\n",
    "            personalities = personalities.flatten()\n",
    "            if _k > 1:\n",
    "                guess = np.bincount(personalities).argmax()\n",
    "            else:\n",
    "                guess = int(personalities)\n",
    "        confMatrixInput.append([guess, result])\n",
    "    for i in confMatrixInput:\n",
    "        confMatrix[i[0], i[1]] += 1\n",
    "    for i in range(16):\n",
    "        tp = tp + confMatrix[i, i]\n",
    "        tn = tn + confMatrix.sum() - confMatrix[i, :].sum() - confMatrix[:, i].sum() + confMatrix[i, i]\n",
    "        fp = fp + confMatrix[i, :].sum() - confMatrix[i, i]\n",
    "        fn = fn + confMatrix[:, i].sum() - confMatrix[i, i]\n",
    "    accuracy = (tp + tn) * 100/(tp + tn + fp + fn)\n",
    "    precision = tp * 100/(tp + fp)\n",
    "    recall = tp * 100/(tp + fn)\n",
    "    calcs.append([accuracy, precision, recall])\n",
    "    \n",
    "print(calcs) \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fd88bfb",
   "metadata": {},
   "source": [
    "    In this assignment both feature normalization and weighted KNN were implemented and can be turned on or off by switching their values to True or False at the start of the code, as well as the value of k to control how many neighbors are considered and the value of k in k-fold crossover to control the percentage of the data that it will be devided for testing and training and the code will print the values accuaracies, precesions and recalls for each fold-crossover so we can determine which part of our data is the best to be taken as a training set.\n",
    "    \n",
    "    The neighbor number (k) can affect the effictiveness of the guessing, picking up a low value of k will cause the guess to be highly sensitive to any value that is close to the tested point, while picking a high value will lower the effect of a single point and might make it a problem landing on far but in range points to the tested point. Because of that, selecting a value in between can be the best choice. Since we didnt choose a k value too big, our guessing kept getting better and better the bigger k got up to 9.\n",
    "    \n",
    "    Through the tests, feature normalization proved to have better results with a small margin of difference, so it has proved that implementing it in the results is the better choice.\n",
    "    \n",
    "    This type of algorithm proved to be good in terms of time taken and results, first deviding the data into k(fold) parts then taking turns for training and testing sets. Then calculating the distances from each training point to the testing point and determening a guess for that point. After checking all the guesses and the results and sorting them into a confusion matrix, the algorithm will calculate the accuracy, precesion, and recall for each fold-crossover. This whole process will take around 15 to 20 mins on a 5-fold crossover, meaning 3-5 mins on each k loop\n",
    "    \n",
    "    Having multiple K-fold crossovers made the algorithm much more accurate by having the choice of maximizing both the efficiency of the training set and testing set. \n",
    "    \n",
    "    The mistakes made by the algorithm are in my opinion usually because of some points being a bit unique and different from the others in a way that even the points close to it can't be a metric for deciding the personality affiliated\n",
    "    \n",
    "    For accuaracies, precesions and recalls, here are the best results from the 5-fold crossover\n",
    "k = 1:\n",
    "Acc = 99.71%, Pre = 97,74%, Rec = 97.41%\n",
    "k = 3:\n",
    "Acc = 99.86%, Pre = 98.89%, Rec = 98.89%\n",
    "k = 5:\n",
    "Acc = 99.87%, Pre = 98.93%, Rec = 98.93%\n",
    "k = 7:\n",
    "Acc = 99.87%, Pre = 98.96%, Rec = 98.96%\n",
    "k = 9:\n",
    "Acc = 99.87%, Pre = 98.98%, Rec = 98.98%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
